/*
Auto-generated by: https://github.com/pmndrs/gltfjsx
Command: npx gltfjsx@6.2.3 public/models/emma.glb -o src/components/Avatar.jsx -k -r public
*/

import { useAnimations, useGLTF } from "@react-three/drei";
import { useFrame } from "@react-three/fiber";
import { button, useControls } from "leva";
import React, { useEffect, useRef, useState } from "react";

import * as THREE from "three";
import { useChat } from "../hooks/useChat";

const facialExpressions = {
  default: {},
  smile: {
    browInnerUp: 0.17,
    eyeSquintLeft: 0.4,
    eyeSquintRight: 0.44,
    noseSneerLeft: 0.1700000727403593,
    noseSneerRight: 0.14000002836874015,
    mouthPressLeft: 0.61,
    mouthPressRight: 0.41000000000000003,
  },
  funnyFace: {
    jawLeft: 0.63,
    mouthPucker: 0.53,
    noseSneerLeft: 1,
    noseSneerRight: 0.39,
    mouthLeft: 1,
    eyeLookUpLeft: 1,
    eyeLookUpRight: 1,
    cheekPuff: 0.9999924982764238,
    mouthDimpleLeft: 0.414743888682652,
    mouthRollLower: 0.32,
    mouthSmileLeft: 0.35499733688813034,
    mouthSmileRight: 0.35499733688813034,
  },
  sad: {
    mouthFrownLeft: 1,
    mouthFrownRight: 1,
    mouthShrugLower: 0.78341,
    browInnerUp: 0.452,
    eyeSquintLeft: 0.72,
    eyeSquintRight: 0.75,
    eyeLookDownLeft: 0.5,
    eyeLookDownRight: 0.5,
    jawForward: 1,
  },
  surprised: {
    eyeWideLeft: 0.5,
    eyeWideRight: 0.5,
    jawOpen: 0.351,
    mouthFunnel: 1,
    browInnerUp: 1,
  },
  angry: {
    browDownLeft: 1,
    browDownRight: 1,
    eyeSquintLeft: 1,
    eyeSquintRight: 1,
    jawForward: 1,
    jawLeft: 1,
    mouthShrugLower: 1,
    noseSneerLeft: 1,
    noseSneerRight: 0.42,
    eyeLookDownLeft: 0.16,
    eyeLookDownRight: 0.16,
    cheekSquintLeft: 1,
    cheekSquintRight: 1,
    mouthClose: 0.23,
    mouthFunnel: 0.63,
    mouthDimpleRight: 1,
  },
  crazy: {
    browInnerUp: 0.9,
    jawForward: 1,
    noseSneerLeft: 0.5700000000000001,
    noseSneerRight: 0.51,
    eyeLookDownLeft: 0.39435766259644545,
    eyeLookUpRight: 0.4039761421719682,
    eyeLookInLeft: 0.9618479575523053,
    eyeLookInRight: 0.9618479575523053,
    jawOpen: 0.9618479575523053,
    mouthDimpleLeft: 0.9618479575523053,
    mouthDimpleRight: 0.9618479575523053,
    mouthStretchLeft: 0.27893590769016857,
    mouthStretchRight: 0.2885543872656917,
    mouthSmileLeft: 0.5578718153803371,
    mouthSmileRight: 0.38473918302092225,
    tongueOut: 0.9618479575523053,
  },
};

const corresponding = {
  A: "viseme_PP",
  B: "viseme_kk",
  C: "viseme_I",
  D: "viseme_AA",
  E: "viseme_O",
  F: "viseme_U",
  G: "viseme_FF",
  H: "viseme_TH",
  X: "viseme_PP",
};

let setupMode = false;

// Variable global para rastrear si el contexto de audio ha sido inicializado
let audioContextInitialized = false;

export function Avatar(props) {
  const { nodes, materials, scene } = useGLTF(
    "/models/emma.glb"
  );

  const { message, onMessagePlayed, chat } = useChat();

  const [lipsync, setLipsync] = useState();
  const audioContextRef = useRef(null);
  const currentAudioRef = useRef(null); // Referencia al audio actual para poder detenerlo
  const isPlayingRef = useRef(false); // Ref para saber si hay un audio reproduci√©ndose (no causa re-renders)
  const processingMessageRef = useRef(null); // Ref para rastrear qu√© mensaje se est√° procesando
  const [audioCompleted, setAudioCompleted] = useState(0); // Estado para forzar re-render cuando termine el audio

  // Mejorar calidad de materiales y texturas
  useEffect(() => {
    scene.traverse((child) => {
      if (child.isMesh && child.material) {
        const materials = Array.isArray(child.material) ? child.material : [child.material];
        materials.forEach((mat) => {
          if (mat && mat.map) {
            // Mejorar calidad de texturas
            mat.map.anisotropy = 16; // Mejor calidad de texturas
            mat.map.minFilter = THREE.LinearMipmapLinearFilter;
            mat.map.magFilter = THREE.LinearFilter;
            mat.map.generateMipmaps = true;
          }
          // Mejorar renderizado
          if (mat) {
            mat.needsUpdate = true;
          }
        });
      }
    });
  }, [scene]);

  // Funci√≥n para inicializar el contexto de audio (necesario para iOS)
  // NO bloquear - solo verificar si ya est√° inicializado
  const initializeAudioContext = async () => {
    // Si ya est√° inicializado globalmente, no hacer nada
    if (window.audioContextInitialized || audioContextInitialized) {
      return true;
    }
    
    // Si no est√° inicializado, intentar inicializar (pero no bloquear)
    try {
      const silentAudio = new Audio('data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQAAAAA=');
      silentAudio.volume = 0.01;
      silentAudio.setAttribute('playsinline', 'true');
      silentAudio.setAttribute('webkit-playsinline', 'true');
      
      await silentAudio.play();
      silentAudio.pause();
      silentAudio.currentTime = 0;
      audioContextInitialized = true;
      return true;
    } catch (e) {
      console.warn('No se pudo inicializar el contexto de audio:', e);
      // Continuar de todos modos
      return true; // Retornar true para no bloquear la reproducci√≥n
    }
  };

  useEffect(() => {
    console.log('üì® useEffect ejecutado - mensaje:', message ? `"${message.text?.substring(0, 50)}..."` : 'null', '| audioCompleted:', audioCompleted);
    if (!message) {
      setAnimation("Idle");
      // Detener cualquier audio que est√© reproduci√©ndose
      if (currentAudioRef.current) {
        currentAudioRef.current.pause();
        currentAudioRef.current.currentTime = 0;
        currentAudioRef.current = null;
        isPlayingRef.current = false;
      }
      processingMessageRef.current = null;
      return;
    }
    
    // Crear un identificador √∫nico para el mensaje (usar audio como ID √∫nico)
    const messageId = message.audio ? message.audio.substring(0, 50) : JSON.stringify(message);
    
    // Si ya estamos procesando este mismo mensaje, no hacer nada
    if (processingMessageRef.current === messageId) {
      console.log('‚ö†Ô∏è Ya se est√° procesando este mensaje (por ID), ignorando...');
      return;
    }
    
    // Si ya hay un audio reproduci√©ndose, esperar a que termine
    if (isPlayingRef.current && currentAudioRef.current) {
      console.log('‚è≥ Esperando a que termine el audio anterior...');
      // NO usar setInterval aqu√≠ - el audio.onended ya manejar√° el siguiente mensaje
      // Simplemente retornar y esperar a que onMessagePlayed() se ejecute
      return; // No hacer nada hasta que termine el audio actual
    }
    
    // Marcar que estamos procesando este mensaje (usar ID en lugar de objeto)
    processingMessageRef.current = messageId;
    console.log(`üé¨ Iniciando reproducci√≥n del mensaje: ${messageId.substring(0, 30)}...`);
    
    setAnimation(message.animation);
    setFacialExpression(message.facialExpression);
    setLipsync(message.lipsync);
    
    const playAudio = async () => {
      // Detener cualquier audio anterior que pueda estar reproduci√©ndose
      if (currentAudioRef.current) {
        currentAudioRef.current.pause();
        currentAudioRef.current.currentTime = 0;
        currentAudioRef.current = null;
      }
      
      // Marcar que estamos reproduciendo
      isPlayingRef.current = true;
      
      // Inicializar contexto de audio si es necesario (para iOS)
      await initializeAudioContext();
      
      // Convertir base64 a Blob (mejor soporte en iOS que data URIs)
      let audioBlob;
      try {
        const audioData = atob(message.audio);
        const audioArray = new Uint8Array(audioData.length);
        for (let i = 0; i < audioData.length; i++) {
          audioArray[i] = audioData.charCodeAt(i);
        }
        audioBlob = new Blob([audioArray], { type: 'audio/mpeg' });
      } catch (decodeError) {
        console.error('Error decodificando base64:', decodeError);
        // Fallback a data URI
        audioBlob = null;
      }
      
      try {
        // Usar Blob URL si est√° disponible, sino usar data URI
        const audioUrl = audioBlob 
          ? URL.createObjectURL(audioBlob)
          : "data:audio/mp3;base64," + message.audio;
        
        // Crear el elemento de audio usando Blob URL (mejor para iOS)
        const audio = new Audio(audioUrl);
        
        // Configuraciones cr√≠ticas para iOS
        audio.setAttribute('playsinline', 'true');
        audio.setAttribute('webkit-playsinline', 'true');
        audio.setAttribute('preload', 'auto');
        audio.crossOrigin = 'anonymous';
        
        // Configurar volumen
        audio.volume = 1.0;
        
        // Para iOS: asegurar que el audio se carga antes de reproducir
        audio.load();
        
        // Esperar a que el audio est√© listo (especialmente importante para iOS y mobile)
        // En mobile, usar un timeout m√°s largo
        const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
        const loadTimeout = isMobile ? 10000 : 5000; // 10s en mobile, 5s en desktop
        
        await new Promise((resolve, reject) => {
          if (audio.readyState >= 2) {
            // HAVE_CURRENT_DATA o superior
            resolve();
          } else {
            const timeout = setTimeout(() => {
              console.warn(`Timeout esperando audio despu√©s de ${loadTimeout}ms`);
              // En mobile, intentar continuar de todos modos
              if (isMobile) {
                resolve(); // Resolver en lugar de rechazar en mobile
              } else {
                reject(new Error('Timeout esperando audio'));
              }
            }, loadTimeout);
            
            audio.addEventListener('canplaythrough', () => {
              clearTimeout(timeout);
              resolve();
            }, { once: true });
            
            audio.addEventListener('canplay', () => {
              // En mobile, tambi√©n aceptar 'canplay' si 'canplaythrough' tarda mucho
              if (isMobile) {
                clearTimeout(timeout);
                resolve();
              }
            }, { once: true });
            
            audio.addEventListener('error', (error) => {
              clearTimeout(timeout);
              reject(error);
            }, { once: true });
          }
        });
        
        // Intentar reproducir el audio
        try {
          // En mobile, asegurar que el audio est√© completamente listo
          if (isMobile && audio.readyState < 3) {
            // Esperar un poco m√°s si no est√° completamente cargado
            await new Promise(resolve => setTimeout(resolve, 100));
          }
          
          const playPromise = audio.play();
          
          // Manejar la promesa de play
          if (playPromise !== undefined) {
            await playPromise;
          }
          
          // En mobile, verificar que realmente est√° reproduci√©ndose
          if (isMobile) {
            await new Promise(resolve => setTimeout(resolve, 50));
            if (audio.paused) {
              console.warn('Audio pausado despu√©s de play() en mobile, reintentando...');
              await audio.play();
            }
          }
          
          setAudio(audio);
          currentAudioRef.current = audio; // Guardar referencia
          
          // Guardar referencias en el closure para limpiarlas despu√©s
          const audioUrlToCleanup = audioUrl;
          const audioBlobToCleanup = audioBlob;
          
          audio.onended = () => {
            console.log('üéµ Audio terminado, limpiando referencias...');
            console.log(`‚úÖ Mensaje completado: ${messageId.substring(0, 30)}...`);
            
            const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
            
            // Guardar referencia al audio antes de limpiar (para verificaci√≥n posterior)
            const audioElement = currentAudioRef.current;
            
            // NO limpiar referencias inmediatamente - esperar a asegurar que el audio termin√≥ completamente
            // Aumentar delays significativamente para evitar que se corten los audios, especialmente el √∫ltimo
            // El modelo eleven_v3 tarda m√°s en generar, as√≠ que necesitamos m√°s tiempo antes de limpiar
            const cleanupDelay = isMobile ? 2000 : 1000; // Delay mucho m√°s largo para asegurar que termine completamente
            const advanceDelay = isMobile ? 800 : 400; // Delay m√°s largo antes de avanzar
            
            setTimeout(() => {
              // Limpiar referencias DESPU√âS de asegurar que el audio termin√≥ completamente
              // Esperar m√°s tiempo para asegurar que el audio realmente termin√≥
              currentAudioRef.current = null;
              isPlayingRef.current = false;
              processingMessageRef.current = null;
              
              // Limpiar Blob URL despu√©s de un delay MUCHO m√°s largo para evitar cortes
              // NO limpiar inmediatamente para evitar que se corte el audio, especialmente el √∫ltimo
              // El delay es largo para asegurar que el audio realmente termin√≥ completamente
              if (audioBlobToCleanup) {
                setTimeout(() => {
                  try {
                    // Verificar que el audio realmente termin√≥ antes de revocar
                    // El audio deber√≠a estar ended cuando onended se dispara, pero verificamos por seguridad
                    if (!audioElement || audioElement.ended || audioElement.readyState >= 2) {
                      URL.revokeObjectURL(audioUrlToCleanup);
                      console.log('üóëÔ∏è Blob URL revocado despu√©s de delay seguro');
                    } else {
                      console.log('‚è≥ Audio a√∫n activo, esperando m√°s tiempo antes de limpiar...');
                      // Esperar un poco m√°s si el audio a√∫n est√° activo
                      setTimeout(() => {
                        try {
                          URL.revokeObjectURL(audioUrlToCleanup);
                          console.log('üóëÔ∏è Blob URL revocado despu√©s de delay adicional');
                        } catch (e) {
                          console.warn('Error revocando Blob URL:', e);
                        }
                      }, 1000);
                    }
                  } catch (e) {
                    console.warn('Error revocando Blob URL:', e);
                  }
                }, cleanupDelay);
              }
              
              // Avanzar al siguiente mensaje despu√©s de un delay adicional m√°s largo
              setTimeout(() => {
                console.log('‚û°Ô∏è Avanzando al siguiente mensaje...');
                setAudioCompleted(prev => prev + 1); // Forzar re-render
                onMessagePlayed();
              }, advanceDelay);
            }, 300); // Delay inicial m√°s largo para asegurar que onended se proces√≥ completamente
          };
          
          // Manejar errores durante la reproducci√≥n
          audio.onerror = (error) => {
            console.error('‚ùå Error en audio durante reproducci√≥n:', error);
            console.error(`Mensaje fallido: ${messageId.substring(0, 30)}...`);
            const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
            const cleanupDelay = isMobile ? 2000 : 1000;
            const advanceDelay = isMobile ? 800 : 400;
            
            setTimeout(() => {
              currentAudioRef.current = null;
              isPlayingRef.current = false;
              processingMessageRef.current = null;
              
              setTimeout(() => {
                if (audioBlob) {
                  try {
                    URL.revokeObjectURL(audioUrl);
                  } catch (e) {
                    console.warn('Error revocando Blob URL (error):', e);
                  }
                }
              }, cleanupDelay);
              
              setTimeout(() => {
                console.log('‚û°Ô∏è Avanzando al siguiente mensaje (error)...');
                setAudioCompleted(prev => prev + 1);
                onMessagePlayed();
              }, advanceDelay);
            }, 300);
          };
          
        } catch (error) {
          console.error('Error reproduciendo audio:', error);
          console.error('Error details:', {
            name: error.name,
            message: error.message,
            code: error.code
          });
          if (audioBlob) URL.revokeObjectURL(audioUrl); // Limpiar Blob URL en caso de error
          
          // Si falla, intentar de nuevo despu√©s de un peque√±o delay
          setTimeout(async () => {
            try {
              // Recrear el Blob URL o usar data URI
              const retryAudioUrl = audioBlob 
                ? URL.createObjectURL(audioBlob)
                : "data:audio/mp3;base64," + message.audio;
              
              const retryAudio = new Audio(retryAudioUrl);
              retryAudio.setAttribute('playsinline', 'true');
              retryAudio.setAttribute('webkit-playsinline', 'true');
              retryAudio.volume = 1.0;
              retryAudio.load();
              
              // Esperar a que est√© listo
              await new Promise((resolve) => {
                if (retryAudio.readyState >= 2) {
                  resolve();
                } else {
                  retryAudio.addEventListener('canplaythrough', () => resolve(), { once: true });
                  retryAudio.addEventListener('error', () => resolve(), { once: true });
                }
              });
              
              const playPromise = retryAudio.play();
              if (playPromise !== undefined) {
                await playPromise;
              }
              setAudio(retryAudio);
              currentAudioRef.current = retryAudio; // Guardar referencia
              retryAudio.onended = () => {
                console.log('üéµ Audio retry terminado, limpiando referencias...');
                const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
                const cleanupDelay = isMobile ? 2000 : 1000;
                const advanceDelay = isMobile ? 800 : 400;
                
                setTimeout(() => {
                  currentAudioRef.current = null;
                  isPlayingRef.current = false;
                  processingMessageRef.current = null;
                  
                  setTimeout(() => {
                    if (audioBlob) {
                      try {
                        URL.revokeObjectURL(retryAudioUrl);
                      } catch (e) {
                        console.warn('Error revocando Blob URL (retry):', e);
                      }
                    }
                  }, cleanupDelay);
                  
                  setTimeout(() => {
                    console.log('‚û°Ô∏è Avanzando al siguiente mensaje (retry)...');
                    setAudioCompleted(prev => prev + 1);
                    onMessagePlayed();
                  }, advanceDelay);
                }, 300);
              };
            } catch (retryError) {
              console.error('Error en segundo intento de reproducir audio:', retryError);
              // A√∫n as√≠, continuar con la animaci√≥n aunque el audio no se reproduzca
              setAudio(audio);
              currentAudioRef.current = null;
              isPlayingRef.current = false;
              processingMessageRef.current = null;
              // Simular que el audio termin√≥ para avanzar (usar duraci√≥n real si est√° disponible)
              const estimatedDuration = audio.duration || 3;
              setTimeout(() => {
                console.log('‚û°Ô∏è Avanzando al siguiente mensaje (timeout)...');
                processingMessageRef.current = null;
                setAudioCompleted(prev => prev + 1);
                onMessagePlayed();
              }, estimatedDuration * 1000);
            }
          }, 300);
        }
      } catch (error) {
        console.error('Error creando audio:', error);
        // Fallback: intentar con data URI directamente
        try {
          const fallbackAudio = new Audio("data:audio/mp3;base64," + message.audio);
          fallbackAudio.setAttribute('playsinline', 'true');
          fallbackAudio.setAttribute('webkit-playsinline', 'true');
          fallbackAudio.volume = 1.0;
          fallbackAudio.load();
          
          await new Promise((resolve) => {
            if (fallbackAudio.readyState >= 2) {
              resolve();
            } else {
              fallbackAudio.addEventListener('canplaythrough', () => resolve(), { once: true });
              fallbackAudio.addEventListener('error', () => resolve(), { once: true });
            }
          });
          
          await fallbackAudio.play();
          setAudio(fallbackAudio);
          currentAudioRef.current = fallbackAudio; // Guardar referencia
          fallbackAudio.onended = () => {
            console.log('üéµ Audio fallback terminado, limpiando referencias...');
            const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
            const advanceDelay = isMobile ? 800 : 400;
            
            setTimeout(() => {
              currentAudioRef.current = null;
              isPlayingRef.current = false;
              processingMessageRef.current = null;
              
              setTimeout(() => {
                console.log('‚û°Ô∏è Avanzando al siguiente mensaje (fallback)...');
                setAudioCompleted(prev => prev + 1);
                onMessagePlayed();
              }, advanceDelay);
            }, 300);
          };
        } catch (fallbackError) {
          console.error('Error en fallback de audio:', fallbackError);
          currentAudioRef.current = null;
          isPlayingRef.current = false;
          processingMessageRef.current = null;
          // Continuar sin audio
          setTimeout(() => {
            console.log('‚û°Ô∏è Avanzando al siguiente mensaje (sin audio)...');
            processingMessageRef.current = null;
            setAudioCompleted(prev => prev + 1);
            onMessagePlayed();
          }, 3000);
        }
      }
    };
    
    playAudio();
  }, [message, onMessagePlayed, audioCompleted]); // Ejecutar cuando message cambia o audio se completa

  const { animations } = useGLTF("/models/animations.glb");

  const group = useRef();
  const { actions, mixer } = useAnimations(animations, group);
  const [animation, setAnimation] = useState(
    animations.find((a) => a.name === "Idle") ? "Idle" : animations[0].name // Check if Idle animation exists otherwise use first animation
  );
  useEffect(() => {
    if (!actions[animation]) {
      console.warn(`Animation "${animation}" not found`);
      return;
    }
    actions[animation]
      .reset()
      .fadeIn(mixer.stats.actions.inUse === 0 ? 0 : 0.5)
      .play();
    return () => {
      if (actions[animation]) {
        actions[animation].fadeOut(0.5);
      }
    };
  }, [animation, actions, mixer]);

  const lerpMorphTarget = (target, value, speed = 0.3) => {
    scene.traverse((child) => {
      if (child.isSkinnedMesh && child.morphTargetDictionary) {
        const index = child.morphTargetDictionary[target];
        if (
          index === undefined ||
          child.morphTargetInfluences[index] === undefined
        ) {
          return;
        }
        // Usar velocidad m√°s r√°pida para lipsync (m√°s responsivo)
        const lerpSpeed = target.startsWith('viseme_') ? Math.max(speed, 0.5) : speed;
        child.morphTargetInfluences[index] = THREE.MathUtils.lerp(
          child.morphTargetInfluences[index],
          value,
          lerpSpeed
        );

        if (!setupMode) {
          try {
            set({
              [target]: value,
            });
          } catch (e) {}
        }
      }
    });
  };

  const [blink, setBlink] = useState(false);
  const [winkLeft, setWinkLeft] = useState(false);
  const [winkRight, setWinkRight] = useState(false);
  const [facialExpression, setFacialExpression] = useState("");
  const [audio, setAudio] = useState();

  useFrame(() => {
    !setupMode &&
      Object.keys(nodes.EyeLeft.morphTargetDictionary).forEach((key) => {
        const mapping = facialExpressions[facialExpression];
        if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
          return; // eyes wink/blink are handled separately
        }
        if (mapping && mapping[key]) {
          lerpMorphTarget(key, mapping[key], 0.2); // M√°s r√°pido para expresiones faciales
        } else {
          lerpMorphTarget(key, 0, 0.2); // M√°s r√°pido para resetear
        }
      });

    lerpMorphTarget("eyeBlinkLeft", blink || winkLeft ? 1 : 0, 0.6);
    lerpMorphTarget("eyeBlinkRight", blink || winkRight ? 1 : 0, 0.6);

    // LIPSYNC
    if (setupMode) {
      return;
    }

    const appliedMorphTargets = [];
    if (message && lipsync && lipsync.mouthCues && lipsync.mouthCues.length > 0) {
      // Usar datos de lip-sync reales si est√°n disponibles
      const currentAudioTime = audio ? audio.currentTime : 0;
      
      // Buscar el mouthCue m√°s cercano al tiempo actual
      let activeCue = null;
      let closestCue = null;
      let closestDistance = Infinity;
      
      for (let i = 0; i < lipsync.mouthCues.length; i++) {
        const mouthCue = lipsync.mouthCues[i];
        const cueStart = mouthCue.start;
        const cueEnd = mouthCue.end;
        const cueCenter = (cueStart + cueEnd) / 2;
        
        // Si estamos dentro del rango del cue, usarlo directamente
        if (currentAudioTime >= cueStart && currentAudioTime <= cueEnd) {
          activeCue = mouthCue;
          break;
        }
        
        // Si no, encontrar el m√°s cercano para transiciones suaves
        const distance = Math.abs(currentAudioTime - cueCenter);
        if (distance < closestDistance) {
          closestDistance = distance;
          closestCue = mouthCue;
        }
      }
      
      const cueToUse = activeCue || closestCue;
      if (cueToUse && corresponding[cueToUse.value]) {
        const visemeTarget = corresponding[cueToUse.value];
        appliedMorphTargets.push(visemeTarget);
        
        // Calcular intensidad basada en la posici√≥n dentro del cue
        let intensity = 1.0;
        if (activeCue) {
          const cueDuration = activeCue.end - activeCue.start;
          const positionInCue = (currentAudioTime - activeCue.start) / cueDuration;
          // Crear una curva suave (m√°s intenso en el centro)
          intensity = Math.sin(positionInCue * Math.PI);
        } else if (closestCue) {
          // Si est√° cerca pero fuera del rango, reducir intensidad
          intensity = Math.max(0, 1 - (closestDistance * 2));
        }
        
        lerpMorphTarget(visemeTarget, intensity, 0.6); // Velocidad m√°s r√°pida para mejor sincronizaci√≥n
      }
    } else if (message && audio && !audio.paused && !audio.ended) {
      // Fallback mejorado: animaci√≥n m√°s natural basada en el audio
      const currentAudioTime = audio.currentTime;
      const audioDuration = audio.duration || 0;
      
      // Solo animar si el audio est√° reproduci√©ndose
      if (audioDuration > 0) {
        // Crear m√∫ltiples frecuencias para movimiento m√°s natural
        const fastWave = Math.sin(currentAudioTime * 12); // Movimiento r√°pido
        const slowWave = Math.sin(currentAudioTime * 4); // Movimiento lento
        const combinedWave = (fastWave + slowWave) / 2;
        
        // Mapear el wave a diferentes visemes
        const visemeSequence = ['A', 'E', 'I', 'O', 'U', 'A', 'E', 'I'];
        const normalizedWave = (combinedWave + 1) / 2; // Normalizar entre 0 y 1
        const visemeIndex = Math.floor(normalizedWave * visemeSequence.length);
        const currentViseme = visemeSequence[Math.min(visemeIndex, visemeSequence.length - 1)];
        const visemeTarget = corresponding[currentViseme];
        
        if (visemeTarget) {
          appliedMorphTargets.push(visemeTarget);
          // Usar una intensidad variable m√°s natural basada en m√∫ltiples ondas
          const intensity = 0.4 + Math.abs(combinedWave) * 0.4;
          lerpMorphTarget(visemeTarget, intensity, 0.5); // M√°s r√°pido para mejor sincronizaci√≥n
        }
      }
    }

    // Resetear todos los visemes que no est√°n activos (m√°s r√°pido para mejor sincronizaci√≥n)
    Object.values(corresponding).forEach((value) => {
      if (appliedMorphTargets.includes(value)) {
        return;
      }
      lerpMorphTarget(value, 0, 0.4); // Velocidad m√°s r√°pida para resetear
    });
  });

  useControls("FacialExpressions", {
    chat: button(() => chat()),
    winkLeft: button(() => {
      setWinkLeft(true);
      setTimeout(() => setWinkLeft(false), 300);
    }),
    winkRight: button(() => {
      setWinkRight(true);
      setTimeout(() => setWinkRight(false), 300);
    }),
    animation: {
      value: animation,
      options: animations.map((a) => a.name),
      onChange: (value) => setAnimation(value),
    },
    facialExpression: {
      options: Object.keys(facialExpressions),
      onChange: (value) => setFacialExpression(value),
    },
    enableSetupMode: button(() => {
      setupMode = true;
    }),
    disableSetupMode: button(() => {
      setupMode = false;
    }),
    logMorphTargetValues: button(() => {
      const emotionValues = {};
      Object.keys(nodes.EyeLeft.morphTargetDictionary).forEach((key) => {
        if (key === "eyeBlinkLeft" || key === "eyeBlinkRight") {
          return; // eyes wink/blink are handled separately
        }
        const value =
          nodes.EyeLeft.morphTargetInfluences[
            nodes.EyeLeft.morphTargetDictionary[key]
          ];
        if (value > 0.01) {
          emotionValues[key] = value;
        }
      });
      console.log(JSON.stringify(emotionValues, null, 2));
    }),
  });

  const [, set] = useControls("MorphTarget", () =>
    Object.assign(
      {},
      ...Object.keys(nodes.EyeLeft.morphTargetDictionary).map((key) => {
        return {
          [key]: {
            label: key,
            value: 0,
            min: nodes.EyeLeft.morphTargetInfluences[
              nodes.EyeLeft.morphTargetDictionary[key]
            ],
            max: 1,
            onChange: (val) => {
              if (setupMode) {
                lerpMorphTarget(key, val, 1);
              }
            },
          },
        };
      })
    )
  );

  useEffect(() => {
    let blinkTimeout;
    const nextBlink = () => {
      blinkTimeout = setTimeout(() => {
        setBlink(true);
        setTimeout(() => {
          setBlink(false);
          nextBlink();
        }, 200);
      }, THREE.MathUtils.randInt(1000, 5000));
    };
    nextBlink();
    return () => clearTimeout(blinkTimeout);
  }, []);

  return (
    <group {...props} dispose={null} ref={group}>
      <primitive object={nodes.Hips} />
      <skinnedMesh
        name="Wolf3D_Body"
        geometry={nodes.Wolf3D_Body.geometry}
        material={materials.Wolf3D_Body}
        skeleton={nodes.Wolf3D_Body.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Bottom"
        geometry={nodes.Wolf3D_Outfit_Bottom.geometry}
        material={materials.Wolf3D_Outfit_Bottom}
        skeleton={nodes.Wolf3D_Outfit_Bottom.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Footwear"
        geometry={nodes.Wolf3D_Outfit_Footwear.geometry}
        material={materials.Wolf3D_Outfit_Footwear}
        skeleton={nodes.Wolf3D_Outfit_Footwear.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Outfit_Top"
        geometry={nodes.Wolf3D_Outfit_Top.geometry}
        material={materials.Wolf3D_Outfit_Top}
        skeleton={nodes.Wolf3D_Outfit_Top.skeleton}
      />
      <skinnedMesh
        name="Wolf3D_Hair"
        geometry={nodes.Wolf3D_Hair.geometry}
        material={materials.Wolf3D_Hair}
        skeleton={nodes.Wolf3D_Hair.skeleton}
      />
      <skinnedMesh
        name="EyeLeft"
        geometry={nodes.EyeLeft.geometry}
        material={materials.Wolf3D_Eye}
        skeleton={nodes.EyeLeft.skeleton}
        morphTargetDictionary={nodes.EyeLeft.morphTargetDictionary}
        morphTargetInfluences={nodes.EyeLeft.morphTargetInfluences}
      />
      <skinnedMesh
        name="EyeRight"
        geometry={nodes.EyeRight.geometry}
        material={materials.Wolf3D_Eye}
        skeleton={nodes.EyeRight.skeleton}
        morphTargetDictionary={nodes.EyeRight.morphTargetDictionary}
        morphTargetInfluences={nodes.EyeRight.morphTargetInfluences}
      />
      <skinnedMesh
        name="Wolf3D_Head"
        geometry={nodes.Wolf3D_Head.geometry}
        material={materials.Wolf3D_Skin}
        skeleton={nodes.Wolf3D_Head.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Head.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Head.morphTargetInfluences}
      />
      <skinnedMesh
        name="Wolf3D_Teeth"
        geometry={nodes.Wolf3D_Teeth.geometry}
        material={materials.Wolf3D_Teeth}
        skeleton={nodes.Wolf3D_Teeth.skeleton}
        morphTargetDictionary={nodes.Wolf3D_Teeth.morphTargetDictionary}
        morphTargetInfluences={nodes.Wolf3D_Teeth.morphTargetInfluences}
      />
    </group>
  );
}

useGLTF.preload("/models/emma.glb");
useGLTF.preload("/models/animations.glb");
